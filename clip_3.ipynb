{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagenette/160px (download: 95.08 MiB, generated: 129.48 MiB, post-processed: Unknown size, total: 129.48 MiB) to /home/gk/.cache/huggingface/datasets/frgfm___imagenette/160px/1.0.0/38929285b8abcae5c1305418e9d8fea5dd6b189bbbd22caba5f5537c7fa0f01f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 99.0M/99.0M [01:52<00:00, 882kB/s] \n",
      "Downloading data: 100%|██████████| 500k/500k [00:00<00:00, 573kB/s]\n",
      "Downloading data: 100%|██████████| 199k/199k [00:00<00:00, 374kB/s]t]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]\n",
      "                                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagenette downloaded and prepared to /home/gk/.cache/huggingface/datasets/frgfm___imagenette/160px/1.0.0/38929285b8abcae5c1305418e9d8fea5dd6b189bbbd22caba5f5537c7fa0f01f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 9469\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imagenette = load_dataset(\n",
    "    'frgfm/imagenette',\n",
    "    '160px',\n",
    "    split='train',\n",
    "    ignore_verifications=False  # set to True if seeing splits Error\n",
    ")\n",
    "imagenette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 568/568 [00:00<00:00, 192kB/s]\n",
      "Downloading: 100%|██████████| 862k/862k [00:01<00:00, 669kB/s] \n",
      "Downloading: 100%|██████████| 525k/525k [00:01<00:00, 389kB/s]  \n",
      "Downloading: 100%|██████████| 2.22M/2.22M [00:04<00:00, 546kB/s] \n",
      "Downloading: 100%|██████████| 389/389 [00:00<00:00, 171kB/s]\n",
      "Downloading: 100%|██████████| 4.19k/4.19k [00:00<00:00, 1.08MB/s]\n",
      "Downloading: 100%|██████████| 316/316 [00:00<00:00, 67.4kB/s]\n",
      "Downloading: 100%|██████████| 605M/605M [07:20<00:00, 1.38MB/s]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "# if you have CUDA or MPS, set it to the active device like this\n",
    "device = \"cuda\" if torch.cuda.is_available() else \\\n",
    "         (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# we initialize a tokenizer, image processor, and the model itself\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a dog running in the snow\"\n",
    "\n",
    "# create transformer-readable tokens\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_emb = model.get_text_features(**inputs)\n",
    "text_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = processor(\n",
    "    text=None,\n",
    "    images=imagenette[0]['image'],\n",
    "    return_tensors='pt'\n",
    ")['pixel_values'].to(device)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_emb = model.get_image_features(image)\n",
    "img_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "# select 100 random image index values\n",
    "sample_idx = np.random.randint(0, len(imagenette)+1, 100).tolist()\n",
    "# extract the image sample from the dataset\n",
    "images = [imagenette[i]['image'] for i in sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:14<00:00,  2.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 512)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 16\n",
    "image_arr = None\n",
    "\n",
    "for i in tqdm(range(0, len(images), batch_size)):\n",
    "    # select batch of images\n",
    "    batch = images[i:i+batch_size]\n",
    "    # process and resize\n",
    "    batch = processor(\n",
    "        text=None,\n",
    "        images=batch,\n",
    "        return_tensors='pt',\n",
    "        padding=True\n",
    "    )['pixel_values'].to(device)\n",
    "    # get image embeddings\n",
    "    batch_emb = model.get_image_features(pixel_values=batch)\n",
    "    # convert to numpy array\n",
    "    batch_emb = batch_emb.squeeze(0)\n",
    "    batch_emb = batch_emb.cpu().detach().numpy()\n",
    "    # add to larger array of all image embeddings\n",
    "    if image_arr is None:\n",
    "        image_arr = batch_emb\n",
    "    else:\n",
    "        image_arr = np.concatenate((image_arr, batch_emb), axis=0)\n",
    "image_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.18991256e-01,  4.21024971e-02, -3.51747051e-02, ...,\n",
       "         3.99638116e-01, -9.78647694e-02,  7.43066221e-02],\n",
       "       [ 6.63230896e-01,  2.78828859e-01, -1.59405559e-01, ...,\n",
       "         4.26265538e-01,  8.68764073e-02, -2.41833195e-01],\n",
       "       [ 3.30069304e-01,  1.47955149e-01,  2.93400437e-02, ...,\n",
       "         8.18970680e-01,  1.49025507e-02,  1.00412294e-01],\n",
       "       ...,\n",
       "       [ 1.70199797e-01, -3.23256552e-02,  7.37652183e-04, ...,\n",
       "         5.50143361e-01,  3.10972035e-01,  2.53675759e-01],\n",
       "       [-9.71966684e-01,  2.79788256e-01, -3.36869240e-01, ...,\n",
       "         1.02926981e+00,  1.21567607e-01, -2.04559550e-01],\n",
       "       [-8.52783561e-01,  1.90348431e-01, -4.11760002e-01, ...,\n",
       "         6.10124826e-01,  4.70097624e-02, -8.22311565e-02]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.291456, 3.392732)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_arr.min(), image_arr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.3697751, 0.45963725)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_arr = image_arr / np.linalg.norm(image_arr, axis=0)\n",
    "image_arr.min(), image_arr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_emb = text_emb.cpu().detach().numpy()\n",
    "scores = np.dot(text_emb, image_arr.T)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.27038944e-01,  9.76806104e-01,  5.27521074e-01,\n",
       "         7.48146653e-01,  1.17575407e+00,  1.38256431e-01,\n",
       "         3.08556700e+00, -9.47804570e-01,  2.96096206e-01,\n",
       "         1.42296898e+00,  2.64722109e-03,  5.62113702e-01,\n",
       "         1.54908824e+00,  5.19008160e-01,  1.27888322e+00,\n",
       "         2.89413738e+00,  5.99182010e-01,  2.46326065e+00,\n",
       "         2.32161164e+00,  5.26606321e-01, -2.15892017e-01,\n",
       "         9.51878488e-01,  1.73784709e+00, -1.94650337e-01,\n",
       "         2.22960281e+00,  4.99060065e-01,  3.87086868e-01,\n",
       "         6.71090066e-01,  7.41404712e-01,  8.30270052e-01,\n",
       "         4.06762362e-02,  2.90264398e-01,  7.15774894e-01,\n",
       "         5.07093191e-01,  1.70750093e+00, -1.57645404e-01,\n",
       "         9.45403934e-01, -3.77126962e-01,  8.90902042e-01,\n",
       "         1.58476162e+00,  1.20106602e+00,  2.00123191e-02,\n",
       "         2.56577396e+00,  7.64355659e-01,  2.33283252e-01,\n",
       "         1.62110567e+00,  1.90073028e-01,  1.22596169e+00,\n",
       "        -8.91583622e-01, -1.02959074e-01,  1.04290307e+00,\n",
       "         2.89909542e-01, -3.88012350e-01,  9.58561063e-01,\n",
       "         8.42425168e-01,  3.72072697e-01,  5.60122967e-01,\n",
       "        -5.25028527e-01,  2.64156294e+00,  7.61591375e-01,\n",
       "         4.38809574e-01,  5.36778629e-01,  1.16743319e-01,\n",
       "         4.15327609e-01,  7.49685764e-01,  4.98749465e-01,\n",
       "        -2.82083809e-01, -3.31326187e-01,  5.39859176e-01,\n",
       "         5.85169435e-01,  3.91677320e-02,  4.10535288e+00,\n",
       "         2.63878226e-01,  9.73030627e-02,  4.09067512e-01,\n",
       "         1.51253414e+00,  2.66777182e+00,  1.16388381e+00,\n",
       "         4.94172096e-01,  6.10957026e-01,  1.29881799e-02,\n",
       "         9.60591435e-01,  4.85201716e-01,  9.49505568e-01,\n",
       "         6.00871563e-01,  2.60276818e+00,  2.71573067e+00,\n",
       "         2.68078995e+00,  2.10587263e+00,  1.15907145e+00,\n",
       "         1.25851285e+00,  1.12633801e+00,  9.97591376e-01,\n",
       "         9.50066805e-01,  1.10157669e-01,  6.57937765e-01,\n",
       "         7.53920317e-01,  1.04386902e+00,  5.01933813e-01,\n",
       "         5.36074042e-01]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0ae4fa97a57fd8d950a2fae74bbb6f77555dfa0963c93066e74e2214e6bf19f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
